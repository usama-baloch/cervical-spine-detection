{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"font-size:30px\">\nðŸ¦´ [inference] Pytorch EfficientNet-v2 single model PL:0.49 ðŸ¦´\n</div>\n\nThis is a bare-bones PyTorch implementation of EfficientNet-v2 based classifier. This is a simple baseline implementation that can be iteratively improved.\n\nHere is a high level explanation of the **inference** flow:\n1. Images are loaded from test folder and transformed to `3x384x384` tensors using the same transformations used in training.\n2. Images are passed to an ensemble of trained `EfficientNet_V2_S`-based multi-label classifiers. The classifier produces probabilities of fractures and probabilities of existence of certain vertebrae in each slice. Note that you can also use a single model `effnetv2` which is trained on all folds and likely performs slightly better than any of the fold-models.\n3. We use a non-parametric model to combine predictions of base models:\n    * For each StudyInstanceUID we first aggregate predictions for each of C1-C7 vertebrae by weighted averaging fracture predictions. Probabilities of vertebrae are used as weights. Example: if we are uncertain that C3 is in the slice (`C3_effnet_vert==0.1`), but we somehow predict high probability of C3 being fractured (`C3_effnet_frac==0.9`), we add it to the final aggregate with low weight `0.9 * 0.1 == 0.09`\n    * We use a simple probability formula to derive `patient_overall` fracture probability. `patient_overall` is a probability of any vertebrae being fractured. It is equal to `1-no-vertebrae-are-fractured`. Under assumption of independence of vertebrae fractures we can derive the following simple equation: $P_{\\text{patient_overall}}=1-\\prod_i{[1-C_i]}$\n","metadata":{"papermill":{"duration":0.008914,"end_time":"2022-08-29T06:34:27.892052","exception":false,"start_time":"2022-08-29T06:34:27.883138","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"try:\n    import pylibjpeg\nexcept:\n    # Offline dependencies:\n    !mkdir -p /root/.cache/torch/hub/checkpoints/\n    !cp ../input/rsna-2022-whl/efficientnet_v2_s-dd5fe13b.pth  /root/.cache/torch/hub/checkpoints/\n\n    !pip install /kaggle/input/rsna-2022-whl/{pydicom-2.3.0-py3-none-any.whl,pylibjpeg-1.4.0-py3-none-any.whl,python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl}\n    !pip install /kaggle/input/rsna-2022-whl/{torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl,torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl}","metadata":{"papermill":{"duration":110.198147,"end_time":"2022-08-29T06:36:18.107292","exception":false,"start_time":"2022-08-29T06:34:27.909145","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:07:02.129247Z","iopub.execute_input":"2022-10-16T10:07:02.129648Z","iopub.status.idle":"2022-10-16T10:08:43.918707Z","shell.execute_reply.started":"2022-10-16T10:07:02.129564Z","shell.execute_reply":"2022-10-16T10:08:43.917812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport re\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pydicom as dicom\nimport torch\nimport torchvision as tv\nfrom sklearn.model_selection import GroupKFold\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torchvision.models.feature_extraction import create_feature_extractor\nfrom tqdm.notebook import tqdm\nimport wandb\n\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\nplt.rcParams['figure.figsize'] = (20, 5)\n\n\n# Effnet\nWEIGHTS = tv.models.efficientnet.EfficientNet_V2_S_Weights.DEFAULT\nRSNA_2022_PATH = '../input/rsna-2022-cervical-spine-fracture-detection'\nTRAIN_IMAGES_PATH = f'{RSNA_2022_PATH}/train_images'\nTEST_IMAGES_PATH = f'{RSNA_2022_PATH}/test_images'\nEFFNET_CHECKPOINTS_PATH = '../input/rsna-2022-base-effnetv2'\n\n# MODEL_NAMES = [f'effnetv2']\n\n# This notebook supports ensembles and single model predictions. Uncomment to switch to ensemble prediction:\nMODEL_NAMES = [f'effnetv2-f{i}' for i in range(5)]\n\n# Common\nFRAC_COLS = [f'C{i}_effnet_frac' for i in range(1, 8)]\nVERT_COLS = [f'C{i}_effnet_vert' for i in range(1, 8)]\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    IS_KAGGLE = True\nexcept:\n    IS_KAGGLE = False\n\n\n# Switch to offline for submission\nos.environ[\"WANDB_MODE\"] = \"offline\"\n\nif os.environ[\"WANDB_MODE\"] == \"online\":\n    if IS_KAGGLE:\n        os.environ['WANDB_API_KEY'] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n\nif not IS_KAGGLE:\n    print('Running locally')\n    RSNA_2022_PATH = '/mnt/rsna2022'\n    TRAIN_IMAGES_PATH = '/mnt/rsna2022/train_images'\n    TEST_IMAGES_PATH = '/mnt/rsna2022/test_images'\n    METADATA_PATH = '/home/vslaykovsky/Downloads/'\n    EFFNET_CHECKPOINTS_PATH = 'frac_checkpoints'\n    os.environ['WANDB_API_KEY'] = 'yourkeyhere'\n\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nif DEVICE == 'cuda':\n    BATCH_SIZE = 64\nelse:\n    BATCH_SIZE = 2","metadata":{"papermill":{"duration":2.969425,"end_time":"2022-08-29T06:36:21.083846","exception":false,"start_time":"2022-08-29T06:36:18.114421","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:43.921433Z","iopub.execute_input":"2022-10-16T10:08:43.922088Z","iopub.status.idle":"2022-10-16T10:08:46.314985Z","shell.execute_reply.started":"2022-10-16T10:08:43.922043Z","shell.execute_reply":"2022-10-16T10:08:46.313787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_df_test():\n    df_test = pd.read_csv(f'{RSNA_2022_PATH}/test.csv')\n\n    if df_test.iloc[0].row_id == '1.2.826.0.1.3680043.10197_C1':\n        # test_images and test.csv are inconsistent in the dev dataset, fixing labels for the dev run.\n        df_test = pd.DataFrame({\n            \"row_id\": ['1.2.826.0.1.3680043.22327_C1', '1.2.826.0.1.3680043.25399_C1', '1.2.826.0.1.3680043.5876_C1'],\n            \"StudyInstanceUID\": ['1.2.826.0.1.3680043.22327', '1.2.826.0.1.3680043.25399', '1.2.826.0.1.3680043.5876'],\n            \"prediction_type\": [\"C1\", \"C1\", \"patient_overall\"]}\n        )\n    return df_test\n\ndf_test = load_df_test()\ndf_test","metadata":{"papermill":{"duration":0.045491,"end_time":"2022-08-29T06:36:21.148473","exception":false,"start_time":"2022-08-29T06:36:21.102982","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:46.318070Z","iopub.execute_input":"2022-10-16T10:08:46.318866Z","iopub.status.idle":"2022-10-16T10:08:46.354241Z","shell.execute_reply.started":"2022-10-16T10:08:46.318831Z","shell.execute_reply":"2022-10-16T10:08:46.353307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_slices = glob.glob(f'{TEST_IMAGES_PATH}/*/*')\ntest_slices = [re.findall(f'{TEST_IMAGES_PATH}/(.*)/(.*).dcm', s)[0] for s in test_slices]\ndf_test_slices = pd.DataFrame(data=test_slices, columns=['StudyInstanceUID', 'Slice']).astype({'Slice': int}).sort_values(['StudyInstanceUID', 'Slice']).reset_index(drop=True)\ndf_test_slices","metadata":{"papermill":{"duration":0.145663,"end_time":"2022-08-29T06:36:21.302098","exception":false,"start_time":"2022-08-29T06:36:21.156435","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:46.355481Z","iopub.execute_input":"2022-10-16T10:08:46.355817Z","iopub.status.idle":"2022-10-16T10:08:46.463456Z","shell.execute_reply.started":"2022-10-16T10:08:46.355787Z","shell.execute_reply":"2022-10-16T10:08:46.462478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n`EffnetDataSet` class returns images of individual slices. It uses a dataframe parameter `df` as a source of slices metadata to locate and load images from `path` folder. It accepts transforms parameter which we set to `WEIGHTS.transforms()`. This is a set of transforms used to pre-train the model on ImageNet dataset.","metadata":{"papermill":{"duration":0.006754,"end_time":"2022-08-29T06:36:21.315661","exception":false,"start_time":"2022-08-29T06:36:21.308907","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"def load_dicom(path):\n    \"\"\"\n    This supports loading both regular and compressed JPEG images. \n    See the first sell with `pip install` commands for the necessary dependencies\n    \"\"\"\n    img=dicom.dcmread(path)\n    img.PhotometricInterpretation = 'YBR_FULL'\n    data = img.pixel_array    \n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data / np.max(data)\n    data=(data * 255).astype(np.uint8)\n    return cv2.cvtColor(data, cv2.COLOR_GRAY2RGB), img\n\n\nim, meta = load_dicom(f'{TRAIN_IMAGES_PATH}/1.2.826.0.1.3680043.10001/1.dcm')\nplt.figure()\nplt.imshow(im)\nplt.title('regular image')\n\nim, meta = load_dicom(f'{TRAIN_IMAGES_PATH}/1.2.826.0.1.3680043.10014/1.dcm')\nplt.figure()\nplt.imshow(im)\nplt.title('jpeg')","metadata":{"papermill":{"duration":0.619183,"end_time":"2022-08-29T06:36:21.941518","exception":false,"start_time":"2022-08-29T06:36:21.322335","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:46.466959Z","iopub.execute_input":"2022-10-16T10:08:46.467504Z","iopub.status.idle":"2022-10-16T10:08:47.035361Z","shell.execute_reply.started":"2022-10-16T10:08:46.467465Z","shell.execute_reply":"2022-10-16T10:08:47.034338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.utils.data as D\n\nclass EffnetDataSet(D.Dataset):    \n    def __init__(self, df, path, transforms=None):\n        super().__init__()\n        self.df = df\n        self.path = path\n        self.transforms = transforms\n        \n    def __getitem__(self, i):\n        path = os.path.join(self.path, self.df.iloc[i].StudyInstanceUID, f'{self.df.iloc[i].Slice}.dcm')        \n        \n        try:\n            img = load_dicom(path)[0] \n            img = np.transpose(img, (2, 0, 1))  # Pytorch uses (batch, channel, height, width) order. Converting (height, width, channel) -> (channel, height, width)\n            if self.transforms is not None:\n                img = self.transforms(torch.as_tensor(img))\n        except Exception as ex:\n            print(ex)\n            return None\n        \n        if 'C1_fracture' in self.df:\n            frac_targets = torch.as_tensor(self.df.iloc[i][['C1_fracture', 'C2_fracture', 'C3_fracture', 'C4_fracture', 'C5_fracture', 'C6_fracture', 'C7_fracture']].astype('float32').values)\n            vert_targets = torch.as_tensor(self.df.iloc[i][['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']].astype('float32').values)\n            frac_targets = frac_targets * vert_targets   # we only enable targets that are visible on the current slice\n            return img, frac_targets, vert_targets\n        return img        \n    \n    def __len__(self):\n        return len(self.df)\n    ","metadata":{"papermill":{"duration":0.023057,"end_time":"2022-08-29T06:36:21.974596","exception":false,"start_time":"2022-08-29T06:36:21.951539","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:47.037037Z","iopub.execute_input":"2022-10-16T10:08:47.037328Z","iopub.status.idle":"2022-10-16T10:08:47.047541Z","shell.execute_reply.started":"2022-10-16T10:08:47.037302Z","shell.execute_reply":"2022-10-16T10:08:47.046525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only X values returned by the test dataset\nds_test = EffnetDataSet(df_test_slices, TEST_IMAGES_PATH,WEIGHTS.transforms())\nX = ds_test[42]","metadata":{"papermill":{"duration":0.04504,"end_time":"2022-08-29T06:36:22.02892","exception":false,"start_time":"2022-08-29T06:36:21.98388","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:47.048985Z","iopub.execute_input":"2022-10-16T10:08:47.049343Z","iopub.status.idle":"2022-10-16T10:08:47.090147Z","shell.execute_reply.started":"2022-10-16T10:08:47.049309Z","shell.execute_reply":"2022-10-16T10:08:47.089345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nIn Pytorch we use create_feature_extractor to access feature layers of pre-existing models. Final flat layer of `efficientnet_v2_s` model is called `flatten`. We'll build our classification layer on top of it. ","metadata":{"papermill":{"duration":0.008956,"end_time":"2022-08-29T06:36:22.047427","exception":false,"start_time":"2022-08-29T06:36:22.038471","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"class EffnetModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        effnet = tv.models.efficientnet_v2_s()\n        self.model = create_feature_extractor(effnet, ['flatten'])\n        self.nn_fracture = torch.nn.Sequential(\n            torch.nn.Linear(1280, 7),\n        )\n        self.nn_vertebrae = torch.nn.Sequential(\n            torch.nn.Linear(1280, 7),\n        )\n\n    def forward(self, x):\n        # returns logits\n        x = self.model(x)['flatten']\n        return self.nn_fracture(x), self.nn_vertebrae(x)\n\n    def predict(self, x):\n        frac, vert = self.forward(x)\n        return torch.sigmoid(frac), torch.sigmoid(vert)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-16T10:08:47.091175Z","iopub.execute_input":"2022-10-16T10:08:47.091942Z","iopub.status.idle":"2022-10-16T10:08:47.099163Z","shell.execute_reply.started":"2022-10-16T10:08:47.091910Z","shell.execute_reply":"2022-10-16T10:08:47.098459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(model, name, path='.'):\n    data = torch.load(os.path.join(path, f'{name}.tph'), map_location=DEVICE)\n    model.load_state_dict(data)\n    return model","metadata":{"papermill":{"duration":0.019322,"end_time":"2022-08-29T06:36:24.180428","exception":false,"start_time":"2022-08-29T06:36:24.161106","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:47.100158Z","iopub.execute_input":"2022-10-16T10:08:47.101055Z","iopub.status.idle":"2022-10-16T10:08:47.111785Z","shell.execute_reply.started":"2022-10-16T10:08:47.101012Z","shell.execute_reply":"2022-10-16T10:08:47.110922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effnet_models = [load_model(EffnetModel(), name, EFFNET_CHECKPOINTS_PATH).to(DEVICE) for name in MODEL_NAMES]","metadata":{"papermill":{"duration":4.116502,"end_time":"2022-08-29T06:36:28.306547","exception":false,"start_time":"2022-08-29T06:36:24.190045","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:47.113133Z","iopub.execute_input":"2022-10-16T10:08:47.113497Z","iopub.status.idle":"2022-10-16T10:08:55.218127Z","shell.execute_reply.started":"2022-10-16T10:08:47.113462Z","shell.execute_reply":"2022-10-16T10:08:55.217318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n1. We run all baseline `effnet_model` on every image from the test set and average outputs \n2. We pass average outputs of the base `effnet_model` to the `lstm_model` to produce the final result for each patient.","metadata":{"papermill":{"duration":0.008912,"end_time":"2022-08-29T06:36:28.325049","exception":false,"start_time":"2022-08-29T06:36:28.316137","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"from typing import List\n\n\ndef predict_effnet(models: List[EffnetModel], ds, max_batches=1e3):\n    dl_test = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count())\n    for m in models:\n        m.eval()\n\n    with torch.no_grad():\n        predictions = []\n        for idx, X in enumerate(tqdm(dl_test, miniters=10)):\n            pred = torch.zeros(len(X), 14).to(DEVICE)\n            for m in models:\n                y1, y2 = m.predict(X.to(DEVICE))\n                pred += torch.concat([y1, y2], dim=1) / len(models)\n            predictions.append(pred)\n            if idx >= max_batches:\n                break\n        return torch.concat(predictions).cpu().numpy()\n\n# Quick test\npredict_effnet([EffnetModel().to(DEVICE)], ds_test, max_batches=2).shape","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":3.84226,"end_time":"2022-08-29T06:36:32.176344","exception":false,"start_time":"2022-08-29T06:36:28.334084","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:55.219406Z","iopub.execute_input":"2022-10-16T10:08:55.220206Z","iopub.status.idle":"2022-10-16T10:08:57.407245Z","shell.execute_reply.started":"2022-10-16T10:08:55.220166Z","shell.execute_reply":"2022-10-16T10:08:57.406241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effnet_pred = predict_effnet(effnet_models, ds_test)\n\ndf_effnet_pred = pd.DataFrame(\n    data=effnet_pred, columns=[f'C{i}_effnet_frac' for i in range(1, 8)] + [f'C{i}_effnet_vert' for i in range(1, 8)]\n)","metadata":{"papermill":{"duration":23.579326,"end_time":"2022-08-29T06:36:55.765273","exception":false,"start_time":"2022-08-29T06:36:32.185947","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:08:57.408667Z","iopub.execute_input":"2022-10-16T10:08:57.409006Z","iopub.status.idle":"2022-10-16T10:31:03.248866Z","shell.execute_reply.started":"2022-10-16T10:08:57.408976Z","shell.execute_reply":"2022-10-16T10:31:03.247772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_pred = pd.concat([df_test_slices, df_effnet_pred], axis=1).sort_values(['StudyInstanceUID', 'Slice'])\ndf_test_pred","metadata":{"papermill":{"duration":0.063127,"end_time":"2022-08-29T06:36:55.844489","exception":false,"start_time":"2022-08-29T06:36:55.781362","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:31:03.250305Z","iopub.execute_input":"2022-10-16T10:31:03.250609Z","iopub.status.idle":"2022-10-16T10:31:03.279032Z","shell.execute_reply.started":"2022-10-16T10:31:03.250579Z","shell.execute_reply":"2022-10-16T10:31:03.278271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_sample_patient(df_pred):\n    patient = np.random.choice(df_pred.StudyInstanceUID)\n    df = df_pred.query('StudyInstanceUID == @patient').reset_index()\n\n    df[[f'C{i}_effnet_frac' for i in range(1, 8)]].plot(\n        title=f'Patient {patient}, fracture prediction',\n        ax=(plt.subplot(1, 2, 1)))\n\n    df[[f'C{i}_effnet_vert' for i in range(1, 8)]].plot(\n        title=f'Patient {patient}, vertebrae prediction',\n        ax=plt.subplot(1, 2, 2)\n    )\n\nplot_sample_patient(df_test_pred)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.629707,"end_time":"2022-08-29T06:36:56.489965","exception":false,"start_time":"2022-08-29T06:36:55.860258","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:31:03.281706Z","iopub.execute_input":"2022-10-16T10:31:03.281992Z","iopub.status.idle":"2022-10-16T10:31:03.898430Z","shell.execute_reply.started":"2022-10-16T10:31:03.281966Z","shell.execute_reply":"2022-10-16T10:31:03.897492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef patient_prediction(df):\n    c1c7 = np.average(df[FRAC_COLS].values, axis=0, weights=df[VERT_COLS].values)\n    pred_patient_overall = 1 - np.prod(1 - c1c7)\n    return pd.Series(data=np.concatenate([[pred_patient_overall], c1c7]), index=['patient_overall'] + [f'C{i}' for i in range(1, 8)])\n\ndf_patient_pred = df_test_pred.groupby('StudyInstanceUID').apply(lambda df: patient_prediction(df))\ndf_patient_pred","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.038032,"end_time":"2022-08-29T06:36:56.54117","exception":false,"start_time":"2022-08-29T06:36:56.503138","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:31:03.899822Z","iopub.execute_input":"2022-10-16T10:31:03.900111Z","iopub.status.idle":"2022-10-16T10:31:03.921774Z","shell.execute_reply.started":"2022-10-16T10:31:03.900084Z","shell.execute_reply":"2022-10-16T10:31:03.920933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub = df_test.copy()\ndf_sub = df_sub.set_index('StudyInstanceUID').join(df_patient_pred)\ndf_sub['fractured'] = df_sub.apply(lambda r: r[r.prediction_type], axis=1)\ndf_sub","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.036645,"end_time":"2022-08-29T06:36:56.590835","exception":false,"start_time":"2022-08-29T06:36:56.55419","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:31:03.923071Z","iopub.execute_input":"2022-10-16T10:31:03.923508Z","iopub.status.idle":"2022-10-16T10:31:03.943273Z","shell.execute_reply.started":"2022-10-16T10:31:03.923471Z","shell.execute_reply":"2022-10-16T10:31:03.942332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[['row_id', 'fractured']].to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.024447,"end_time":"2022-08-29T06:36:56.629233","exception":false,"start_time":"2022-08-29T06:36:56.604786","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-10-16T10:31:03.944671Z","iopub.execute_input":"2022-10-16T10:31:03.945217Z","iopub.status.idle":"2022-10-16T10:31:03.954344Z","shell.execute_reply.started":"2022-10-16T10:31:03.945181Z","shell.execute_reply":"2022-10-16T10:31:03.953514Z"},"trusted":true},"execution_count":null,"outputs":[]}]}